{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b3df1be-78bd-40f0-9bce-cbbec6303eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T06:24:54.934270Z",
     "iopub.status.busy": "2024-06-08T06:24:54.933792Z",
     "iopub.status.idle": "2024-06-08T06:24:54.943236Z",
     "shell.execute_reply": "2024-06-08T06:24:54.942742Z",
     "shell.execute_reply.started": "2024-06-08T06:24:54.934249Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import entropy, ks_2samp, f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def calculate_divergences_and_ks(df, smoothing=1e-10):\n",
    "    \"\"\"Calculates normalized divergences (JSD, KL, GJS, SGJS) and KS test for methylation data.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame with 'methylated' and 'unmethylated' columns.\n",
    "        smoothing (float, optional): Smoothing factor to add to counts. Defaults to 1e-10.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Normalized divergences (JSD, KL, GJS, SGJS) and KS test statistic and p-value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter out rows with zero coverage\n",
    "    #df = df[df['coverage'] > 0].copy()\n",
    "\n",
    "    # Drop rows where both methylated and unmethylated are zero \n",
    "    df = df[(df['methylated'] != 0) | (df['unmethylated'] != 0)].copy()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    # Add smoothing to avoid division by zero issues\n",
    "    df[\"methylated\"] += smoothing\n",
    "    df[\"unmethylated\"] += smoothing\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    total = df[\"methylated\"] + df[\"unmethylated\"]\n",
    "    p = df[\"methylated\"] / total\n",
    "    q = df[\"unmethylated\"] / total\n",
    "    \n",
    "    # Divergence calculations (with normalizations)\n",
    "    normalized_js_divergence = float(jensenshannon(p, q, base=2))  # JSD\n",
    "\n",
    "    normalized_kl_divergence = float(entropy(p, q, base=2) - entropy(p, base=2))  # KLD\n",
    "\n",
    "    # Geometric Jensen-Shannon Divergence\n",
    "    normalized_gjs_divergence = float(np.sqrt(\n",
    "        0.5 * (np.sqrt(entropy(p, q, base=2)) + np.sqrt(entropy(q, p, base=2)))\n",
    "    ) / np.sqrt(np.log(2)))\n",
    "    \n",
    "    # Symmetric Geometric Jensen-Shannon Divergence\n",
    "    m = (p + q) / 2\n",
    "    normalized_sgjs_divergence = float( 0.5 * (\n",
    "        np.sqrt(jensenshannon(p, m, base=2)) + np.sqrt(jensenshannon(q, m, base=2))\n",
    "    ) / np.sqrt(np.log(2)))\n",
    "\n",
    "    # Kolmogorov-Smirnov Test\n",
    "    ks_statistic, ks_pvalue = ks_2samp(df[\"methylated\"], df[\"unmethylated\"])  # KST\n",
    "\n",
    "    return normalized_js_divergence, normalized_kl_divergence, normalized_gjs_divergence, normalized_sgjs_divergence, ks_statistic, ks_pvalue\n",
    "\n",
    "def read_bismark_file(filename):\n",
    "    \"\"\"Reads Bismark methylation data from a file.\"\"\"\n",
    "    column_names = [\"chr\", \"start\", \"end\", \"coverage\", \"methylated\", \"unmethylated\"]\n",
    "    df = pd.read_csv(filename, sep='\\t', header=None, names=column_names, compression='gzip')\n",
    "\n",
    "    # Convert columns to correct data types\n",
    "    df['start'] = pd.to_numeric(df['start'])\n",
    "    df['end'] = pd.to_numeric(df['end'])\n",
    "    df['coverage'] = pd.to_numeric(df['coverage'])\n",
    "    df['methylated'] = pd.to_numeric(df['methylated'])\n",
    "    df['unmethylated'] = pd.to_numeric(df['unmethylated'])\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data(results_df):\n",
    "    \"\"\"Cleans the results DataFrame by removing file extensions from sample names.\"\"\"\n",
    "    results_df['Sample'] = results_df['Sample'].astype(str).str.replace('.bedgraph.gz', '', regex=False)\n",
    "    return results_df\n",
    "\n",
    "def perform_anova(results_df, sample_name, divergence_column):\n",
    "    \"\"\"Performs ANOVA and Tukey's HSD test on a specific divergence column for a given sample.\"\"\"\n",
    "    sample_df = results_df[results_df['Sample'] == sample_name]\n",
    "    groups = sample_df.groupby('Context')[divergence_column].apply(list)\n",
    "\n",
    "    # Check if there are enough groups for ANOVA (at least 2)\n",
    "    if len(groups) < 2:\n",
    "        print(f\"Skipping ANOVA for sample {sample_name} - Not enough groups for {divergence_column}\")\n",
    "        return None, None\n",
    "\n",
    "    statistic, pvalue = f_oneway(*groups)\n",
    "\n",
    "    if pvalue < 0.05:  # Significance level\n",
    "        tukey_results = pairwise_tukeyhsd(sample_df[divergence_column], sample_df['Context'])\n",
    "        return statistic, tukey_results\n",
    "    else:\n",
    "        return statistic, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "299cb9e2-658e-418a-b603-110baef0487c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T06:25:37.606768Z",
     "iopub.status.busy": "2024-06-08T06:25:37.606479Z",
     "iopub.status.idle": "2024-06-08T06:25:40.173178Z",
     "shell.execute_reply": "2024-06-08T06:25:40.172395Z",
     "shell.execute_reply.started": "2024-06-08T06:25:37.606749Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample directory: /shares/grossniklaus.botinst.uzh/dkt/projects/meth1000/analysis/09_split_cov_chr/output/SRX1665021_se\n",
      "['CHG', 'SRX1665021', 'se.bismark', 'chr', '3.cov.gz']\n",
      "['CpG', 'SRX1665021', 'se.bismark', 'chr', '1.cov.gz']\n",
      "['CHH', 'SRX1665021', 'se.bismark', 'chr', '1.cov.gz']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 30\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(parts)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#logging.info(f\"Processing file: {filename}\")\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mread_bismark_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m divergences \u001b[38;5;241m=\u001b[39m calculate_divergences_and_ks(df)\n\u001b[1;32m     33\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     34\u001b[0m     {\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     }\n\u001b[1;32m     45\u001b[0m )\n",
      "Cell \u001b[0;32mIn[61], line 68\u001b[0m, in \u001b[0;36mread_bismark_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Reads Bismark methylation data from a file.\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoverage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethylated\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munmethylated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 68\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Convert columns to correct data types\u001b[39;00m\n\u001b[1;32m     71\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/data/conda/envs/environment_shannon/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/conda/envs/environment_shannon/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/conda/envs/environment_shannon/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/data/conda/envs/environment_shannon/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/data/conda/envs/environment_shannon/lib/python3.12/gzip.py:337\u001b[0m, in \u001b[0;36mGzipFile.read1\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    336\u001b[0m     size \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data/conda/envs/environment_shannon/lib/python3.12/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m~/data/conda/envs/environment_shannon/lib/python3.12/gzip.py:535\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mneeds_input:\n\u001b[1;32m    534\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(READ_BUFFER_SIZE)\n\u001b[0;32m--> 535\u001b[0m     uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    537\u001b[0m     uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Data processing pipeline\n",
    "data_directory = \"/shares/grossniklaus.botinst.uzh/dkt/projects/meth1000/analysis/09_split_cov_chr/output\"\n",
    "results = []\n",
    "\n",
    "# Get all sample subdirectories\n",
    "sample_directories = [d for d in os.listdir(data_directory) if os.path.isdir(os.path.join(data_directory, d))]\n",
    "\n",
    "# Iterate through files\n",
    "for sample_dir in sample_directories:\n",
    "    sample_path = os.path.join(data_directory, sample_dir)\n",
    "\n",
    "    print(f\"Processing sample directory: {sample_path}\")\n",
    "\n",
    "    # Iterate through files in each sample directory\n",
    "    for file_path in glob.glob(os.path.join(sample_path, \"*_chr_*.cov.gz\")):\n",
    "        try:\n",
    "            filename = os.path.basename(file_path)\n",
    "            parts = filename.split(\"_\")\n",
    "\n",
    "            context_type = parts[0]\n",
    "            sample_name = parts[1] + \"_\" + parts[2].split(\".\")[0] \n",
    "            chromosome = parts[-1].split(\".\")[0]\n",
    "\n",
    "            print(parts)\n",
    "\n",
    "            #logging.info(f\"Processing file: {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "            df = read_bismark_file(file_path)\n",
    "            divergences = calculate_divergences_and_ks(df)\n",
    "    \n",
    "            results.append(\n",
    "                {\n",
    "                    \"Sample\": sample_name,\n",
    "                    \"Chromosome\": chromosome,\n",
    "                    \"Context\": context_type,\n",
    "                    \"JS Divergence\": divergences[0],  \n",
    "                    \"KL Divergence\": divergences[1],\n",
    "                    \"GJS Divergence\": divergences[2],\n",
    "                    \"SGJS Divergence\": divergences[3],\n",
    "                    \"KS Statistic\": divergences[4],\n",
    "                    \"KS P-value\": divergences[5]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "# Create a DataFrame from results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Clean data\n",
    "#results_df = clean_data(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "\n",
    "results_df.to_csv(\"/shares/grossniklaus.botinst.uzh/eharputluoglu/calculation_output/methylation_analysis_results_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180fff6-64d4-4623-860a-8d389e6a8246",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_shannon",
   "language": "python",
   "name": "environment_shannon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
