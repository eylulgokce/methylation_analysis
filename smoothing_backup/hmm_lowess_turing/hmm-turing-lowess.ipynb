{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9b471-a274-4b4b-9251-3988c74b9cb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-27T16:32:48.940846Z",
     "iopub.status.busy": "2024-06-27T16:32:48.940372Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip\n",
    "import time\n",
    "import zipfile\n",
    "from scipy.stats import entropy, ks_2samp, f_oneway\n",
    "from scipy.special import rel_entr\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "# Function to calculate KL divergence\n",
    "def kl_divergence(p, q):\n",
    "    return np.sum(rel_entr(p, q))\n",
    "\n",
    "# Function to calculate geometric Jensen-Shannon divergence\n",
    "def geometric_jsd(p, q):\n",
    "    jsd = jensenshannon(p, q)\n",
    "    return np.sqrt(jsd)\n",
    "\n",
    "# Function to perform Kolmogorov-Smirnov test\n",
    "def ks_test(p, q):\n",
    "    ks_result = ks_2samp(p, q)\n",
    "    return ks_result.statistic, ks_result.pvalue\n",
    "\n",
    "# Smoothing functions\n",
    "def moving_average(data, window_size=3):\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def gaussian_smoothing(data, sigma=10):  # Adjusted sigma for smaller range\n",
    "    return np.exp(-np.square(data - np.mean(data)) / (2 * sigma**2)) / (sigma * np.sqrt(2 * np.pi))\n",
    "\n",
    "def laplace_smoothing(data, alpha=1e-10):  # Much smaller alpha for more realistic smoothing\n",
    "    return (data + alpha) / (np.sum(data) + alpha * len(data))\n",
    "\n",
    "def good_turing_smoothing(data):\n",
    "    unique, counts = np.unique(data, return_counts=True)\n",
    "    freq_of_freqs = np.bincount(counts)\n",
    "    smoothed = np.array([((c+1) * (freq_of_freqs[c+1]/freq_of_freqs[c]) if c+1 < len(freq_of_freqs) else c) for c in counts])\n",
    "    return smoothed / np.sum(smoothed)\n",
    "\n",
    "def lowess_smoothing(data, frac=0.1, downsample_factor=10):\n",
    "    n = len(data)\n",
    "    \n",
    "    # Downsample the data\n",
    "    downsampled_indices = np.arange(0, n, downsample_factor)\n",
    "    downsampled_data = data[downsampled_indices]\n",
    "    \n",
    "    smoothed_downsampled_data = lowess(downsampled_data, downsampled_indices, frac=frac)[:, 1]\n",
    "    \n",
    "    interpolation_function = interp1d(downsampled_indices, smoothed_downsampled_data, kind='linear', fill_value=\"extrapolate\")\n",
    "    smoothed_data = interpolation_function(np.arange(n))\n",
    "    \n",
    "    return smoothed_data\n",
    "\n",
    "def hmm_smoothing(data, n_components=2):\n",
    "    model = GaussianMixture(n_components=n_components, max_iter=100)  # Reduced max_iter for faster computation\n",
    "    data_reshaped = data.reshape(-1, 1)\n",
    "    model.fit(data_reshaped)\n",
    "    smoothed = model.predict_proba(data_reshaped)[:, 1]\n",
    "    return smoothed\n",
    "\n",
    "# Function to apply smoothing\n",
    "def apply_smoothing(data, method):\n",
    "    if method == 'moving_average':\n",
    "        return moving_average(data)\n",
    "    elif method == 'gaussian':\n",
    "        return gaussian_smoothing(data)\n",
    "    elif method == 'laplace':\n",
    "        return laplace_smoothing(data)\n",
    "    elif method == 'good_turing':\n",
    "        return good_turing_smoothing(data)\n",
    "    elif method == 'lowess':\n",
    "        return lowess_smoothing(data)\n",
    "    elif method == 'hmm':\n",
    "        return hmm_smoothing(data)\n",
    "    elif method == 'none':\n",
    "        return data\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown smoothing method: {method}\")\n",
    "\n",
    "# Function to calculate divergence metrics\n",
    "def divergence_calculations(data, smoothing='laplace'):\n",
    "    # Filter out zero values in methylated and unmethylated counts\n",
    "    data = data[(data['methylated'] > 0) & (data['unmethylated'] > 0)]\n",
    "\n",
    "    # Calculate the statistics for the entire dataset\n",
    "    p = data[['methylated', 'unmethylated']].values.flatten().astype(np.float64)\n",
    "    q = data[['unmethylated', 'methylated']].values.flatten().astype(np.float64)\n",
    "\n",
    "    # Apply smoothing if not 'none'\n",
    "    if smoothing != 'none':\n",
    "        p = apply_smoothing(p, smoothing)\n",
    "        q = apply_smoothing(q, smoothing)\n",
    "\n",
    "    # Normalize p and q\n",
    "    p /= p.sum()\n",
    "    q /= q.sum()\n",
    "\n",
    "    kl = kl_divergence(p, q)\n",
    "    js = jensenshannon(p, q)\n",
    "    gjs = geometric_jsd(p, q)\n",
    "    ks_stat, ks_pvalue = ks_test(p, q)\n",
    "\n",
    "    dataset_result = {\n",
    "        'entropy': entropy(p),\n",
    "        'relative_entropy': kl,  # KL divergence is relative entropy\n",
    "        'jsd': js,\n",
    "        'geometric_jsd': gjs,\n",
    "        'kolmogorov_smirnov_stat': ks_stat,\n",
    "        'kolmogorov_smirnov_pvalue': ks_pvalue\n",
    "    }\n",
    "    return dataset_result\n",
    "\n",
    "# Function to process input files\n",
    "def process_file(file_path):\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        df = pd.read_csv(f, sep='\\t', header=None, names=['chr', 'start', 'end', 'methylated', 'unmethylated', 'percentage'])\n",
    "    return df\n",
    "\n",
    "# Function to analyze samples\n",
    "def analyze_samples(base_dir, smoothing_methods, save_interval=100):\n",
    "    results = {}\n",
    "    times = {method: [] for method in smoothing_methods}\n",
    "\n",
    "    file_counter = 0\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.cov.gz'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                #print(f\"Processing file: {file_path}\")\n",
    "                data = process_file(file_path)\n",
    "                \n",
    "                for method in smoothing_methods:\n",
    "                    #print(f\"Testing method: {method}\")\n",
    "                    start_time = time.time()\n",
    "                    result = divergence_calculations(data, smoothing=method)\n",
    "                    end_time = time.time()\n",
    "                    elapsed_time = end_time - start_time\n",
    "                    times[method].append(elapsed_time)\n",
    "                    results[(file, method)] = result\n",
    "                \n",
    "                file_counter += 1\n",
    "\n",
    "                # Save interim results periodically\n",
    "                if file_counter % save_interval == 0:\n",
    "                    save_results(results, times, 'interim_results.zip')\n",
    "\n",
    "    # Save the final results\n",
    "    save_results(results, times, 'final_results.zip')\n",
    "    return results, times\n",
    "\n",
    "# Function to save results to a zip file\n",
    "def save_results(results, times, zip_file):\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df.to_csv('results.csv', index=True)\n",
    "    \n",
    "    times_df = pd.DataFrame(times)\n",
    "    times_df.to_csv('times.csv', index=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_file, 'w') as zipf:\n",
    "        zipf.write('results.csv')\n",
    "        zipf.write('times.csv')\n",
    "\n",
    "    os.remove('results.csv')\n",
    "    os.remove('times.csv')\n",
    "\n",
    "# Function to plot results\n",
    "def plot_results(results, times):\n",
    "    methods = list(set(key[1] for key in results.keys()))\n",
    "    files = list(set(key[0] for key in results.keys()))\n",
    "\n",
    "    entropies = {method: [] for method in methods}\n",
    "    kl_divergences = {method: [] for method in methods}\n",
    "    jsds = {method: [] for method in methods}\n",
    "    geometric_jsds = {method: [] for method in methods}\n",
    "    ks_statistics = {method: [] for method in methods}\n",
    "    ks_pvalues = {method: [] for method in methods}\n",
    "\n",
    "    for file in files:\n",
    "        for method in methods:\n",
    "            result = results.get((file, method))\n",
    "            if result:\n",
    "                entropies[method].append(result['entropy'])\n",
    "                kl_divergences[method].append(result['relative_entropy'])\n",
    "                jsds[method].append(result['jsd'])\n",
    "                geometric_jsds[method].append(result['geometric_jsd'])\n",
    "                ks_statistics[method].append(result['kolmogorov_smirnov_stat'])\n",
    "                ks_pvalues[method].append(result['kolmogorov_smirnov_pvalue'])\n",
    "\n",
    "    x = np.arange(len(files))\n",
    "\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(20, 20))  # Increased figure size\n",
    "\n",
    "    for method in methods:\n",
    "        axs[0, 0].plot(x, entropies[method], label=method)\n",
    "        axs[0, 1].plot(x, kl_divergences[method], label=method)\n",
    "        axs[1, 0].plot(x, jsds[method], label=method)\n",
    "        axs[1, 1].plot(x, geometric_jsds[method], label=method)\n",
    "        axs[2, 0].plot(x, ks_statistics[method], label=method)\n",
    "        axs[2, 1].plot(x, ks_pvalues[method], label=method)\n",
    "\n",
    "    axs[0, 0].set_title('Entropy')\n",
    "    axs[0, 1].set_title('KL Divergence')\n",
    "    axs[1, 0].set_title('Jensen-Shannon Divergence')\n",
    "    axs[1, 1].set_title('Geometric JSD')\n",
    "    axs[2, 0].set_title('Kolmogorov-Smirnov Statistic')\n",
    "    axs[2, 1].set_title('Kolmogorov-Smirnov P-value')\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set_xticks(x[::max(1, len(x))])  # Show a subset of labels to avoid clutter\n",
    "        ax.set_xticklabels(files[::max(1, len(x))], rotation=90)  # Rotate labels for better readability\n",
    "        ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the times for each method\n",
    "    fig, ax = plt.subplots(figsize=(15, 7))\n",
    "    avg_times = {method: np.mean(times[method]) for method in times}\n",
    "    ax.bar(avg_times.keys(), avg_times.values())\n",
    "    ax.set_title('Average Processing Time by Smoothing Method')\n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ANOVA test to compare times between different smoothing methods\n",
    "    time_values = [times[method] for method in times]\n",
    "    anova_result = f_oneway(*time_values)\n",
    "    print(\"ANOVA test result:\", anova_result)\n",
    "\n",
    "# Sample directory path\n",
    "base_dir = '/home/eharpu/methylation_analysis/samples_testing'\n",
    "smoothing_methods = ['none', 'gaussian', 'good_turing', 'hmm', 'lowess']\n",
    "results, times = analyze_samples(base_dir, smoothing_methods)\n",
    "plot_results(results, times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb78600-d542-4e77-a131-c8a794279316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment_shannon",
   "language": "python",
   "name": "environment_shannon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
